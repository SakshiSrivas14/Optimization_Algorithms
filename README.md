# Numerical_Optimization-Algorithms
Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.
My implementation of GD algorithms:
Batch GD , Mini-Batch , Stochastic , Momentum , Nesterov accelerated GD (NAG) , Adagrad , RMSProp , Adam  

